# Logstash 및 플러그인 기반 데이터 처리 파이프라인

Logstash는 플러그인 기반의 오픈소스 데이터 처리 파이프라인 도구입니다.

## 1. 특징

1. **플러그인 형태의 구성**:
   - Logstash의 파이프라인을 구성하는 각 요소는 모두 플러그인 형태로 제공됩니다.

2. **다양한 데이터 처리**:
   - JSON, XML과 같은 구조화된 텍스트뿐만 아니라 다양한 형태의 데이터를 입력받아 가공 및 저장 가능합니다.
   - 특히, 시계열 데이터나 이벤트 데이터를 가공하는 데 최적화되어 있습니다.

3. **높은 처리 속도와 안정성**:
   - 자체 내장된 메모리 및 파일 기반 큐를 사용하여 처리 속도와 안정성을 제공합니다.

4. **오류 관리**:
   - Elasticsearch 장애 상황에 대응하기 위한 재시도 로직을 내장하고 있으며, 오류 발생 시 해당 문서를 따로 보관하는 데드 레터 큐를 지원합니다.

## 2. 파이프라인

Logstash의 파이프라인은 데이터를 입력받아 실시간으로 변경하고 이를 다른 시스템에 전달하는 역할을 합니다. 파이프라인은 세 가지 구성 요소로 이루어져 있습니다.

### 2.1 입력 (Input)

입력 플러그인은 다양한 데이터 형태를 인식하고 처리할 수 있게 합니다. 주로 사용하는 입력 플러그인은 다음과 같습니다:
- `file`
- `syslog`
- `kafka`
- `jdbc`

#### 입력 예제

```plaintext
input {
  file {
    path => "C:/elasticsearch-7.10.1/logs/elasticsearch.log"
    start_position => "beginning" # 최초 파일을 발견했을 때 읽어올 위치
  }
}

output {
  stdout {}
}
```

### 2.2 필터 (Filter)

필터는 입력 데이터를 의미 있는 데이터로 구조화하는 역할을 합니다. 대표적인 필터 플러그인은 다음과 같습니다:
- `grok`
- `dissect`
- `mutate`
- `date`

#### 필터 예제

```plaintext
input {
  file {
    path => "C:/elasticsearch-7.10.1/logs/elasticsearch.log"
    start_position => "beginning" # 최초 파일을 발견했을 때 읽어올 위치
  }
}

filter {
  mutate {
    split => { "message" => " " } # 문자열을 자르는 기능
  }
  if [LEVEL] == "INFO" {
    dissect {
      mapping => {"message" => "[%{timestamp}] ..."}
    }
  }
  date {
    match => [ "timestamp", "YYYY-MM-dd HH:mm", "yyyy/MM/dd HH:mm:ss" ]
    target => "new_timestamp"
    timezone => "UTC"
  }
}

output {
  stdout {}
}
```

#### 필터의 공통 옵션

- `add_field`
- `add_tag`
- `enable_metric`
- `id`
- `remove_field`
- `remove_tag`

### 2.3 출력 (Output)

출력은 파이프라인의 마지막 단계로, 가공된 데이터를 지정된 대상으로 내보냅니다. 주요 출력 플러그인은 다음과 같습니다:
- `file`
- `elasticsearch`

#### 출력 예제

```plaintext
input {
  file {
    path => "C:/elasticsearch-7.10.1/logs/elasticsearch.log"
    start_position => "beginning" # 최초 파일을 발견했을 때 읽어올 위치
  }
}

filter {
  mutate {
    split => { "message" => " " } # 문자열을 자르는 기능
  }
  if [LEVEL] == "INFO" {
    dissect {
      mapping => {"message" => "[%{timestamp}] ..."}
    }
  }
  date {
    match => [ "timestamp", "YYYY-MM-dd HH:mm", "yyyy/MM/dd HH:mm:ss" ]
    target => "new_timestamp"
    timezone => "UTC"
  }
}

output {
  file {
    path => "C:/logstash-7.10.1/config/filter-example.log"
  }
  elasticsearch {
    index => "output"
  }
}
```

## 3. 코덱 (Codec)

코덱은 입력/출력 과정에서 데이터를 적절한 형태로 변환하는 스트림 필터입니다. 독립적으로 동작하지 않고 입력과 출력 과정에 사용됩니다.

#### 코덱 예제

```plaintext
input {
  file {
    path => "C:/elasticsearch-7.10.1/logs/elasticsearch.log"
    start_position => "beginning" # 최초 파일을 발견했을 때 읽어올 위치
    codec => "json"
  }
}

filter {
  mutate {
    split => { "message" => " " } # 문자열을 자르는 기능
  }
  if [LEVEL] == "INFO" {
    dissect {
      mapping => {"message" => "[%{timestamp}] ..."}
    }
  }
  date {
    match => [ "timestamp", "YYYY-MM-dd HH:mm", "yyyy/MM/dd HH:mm:ss" ]
    target => "new_timestamp"
    timezone => "UTC"
  }
}

output {
  file {
    path => "C:/logstash-7.10.1/config/filter-example.log"
  }
  elasticsearch {
    index => "output"
  }
}
```

## 4. 다중 파이프라인 (Multiple Pipelines)

여러 서버에서 데이터를 처리할 경우, 하나의 복잡한 파이프라인 대신 다중 파이프라인을 사용할 수 있습니다. 이를 통해 유지보수성을 높이고 구성 관리가 용이해집니다.

#### 다중 파이프라인 예제

- 파이프라인 1

```plaintext
input {
  kafka {
    # Kafka 관련 설정
  }
}

filter {
  # 필터 관련 설정
}

output {
  file {
    path => "C:/logstash-7.10.1/config/kafka-output.log"
  }
}
```

- 파이프라인 2

```plaintext
input {
  file {
    path => "C:/elasticsearch-7.10.1/logs/elasticsearch.log"
    start_position => "beginning"
    codec => "json"
  }
}

filter {
  # 필터 관련 설정
}

output {
  if [LEVEL] == "INFO" {
    elasticsearch {
      index => "INFO"
    }
  } else {
    elasticsearch {
      index => "ELSE"
    }
  }
}
```

`pipeline.yml` 파일을 사용하여 다중 파이프라인 설정을 활성화하면 각각의 파이프라인이 독립적으로 동작합니다.

## 5. 모니터링

Logstash 상태를 확인하는 방법:
1. **API 활용**:
   - Logstash가 제공하는 API를 사용하여 상태를 확인할 수 있습니다.
2. **Kibana 통합**:
   - 모니터링 기능을 활성화하여 Kibana에서 대시보드 형태로 통계를 확인할 수 있습니다.

---

